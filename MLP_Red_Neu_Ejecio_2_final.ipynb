{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otrilas/IA/blob/main/MLP_Red_Neu_Ejecio_2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3vcc9MQayqa"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/025_mlp_framework/mlp_framework.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiKMT3dUayqd"
      },
      "source": [
        "# El Perceptrón Multicapa - Nuestro propio Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpSFHsuuayqe"
      },
      "source": [
        "En el [post](https://sensioai.com/blog/024_mlp_clasificacion) anterior completamos una implementación del Perceptrón Multicapa capaz de llevar a cabo tareas de `regresión` y `clasificación`. Sin embargo, esta implementación tiene muchas limitaciones. Por ejemplo, sólo sirve para un perceptrón multicapa con una capa oculta con funciones de activación de tipo `relu`, entre otras. Aún así, programar todos los elementos envueltos en un arquitectura en concreto es muy útil para entender qué está pasando dentro de la `red neuronal` en todo momento. En este post vamos a mejorar la implementación para resolver estas limitaciones, haciendo nuestro propio framework inspirado en [Pytorch](https://pytorch.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mazSFzc9ayqe"
      },
      "source": [
        "## Diseño de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZOKBerwayqf"
      },
      "source": [
        "Nuestro objetivo es el de ser capaces de resolver los problemas que hemos visto hasta ahora con una interfaz similar a la siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kdCri47ayqg"
      },
      "source": [
        "En primer lugar, queremos definir nuestro `MLP` como una secuencia de capas, las cuales pasaremos como una lista a la clase `MLP`. Como puedes ver, de esta manera, podemos usar diferentes funciones de activación simplemente cambiando la clase utilizada. Después, definiremos los objetos `optimizer` y `loss` como clases particulares de manera que podamos, de manera totalmente transparente, utilizar diferentes funciones de pérdida o algoritmos de optimización simplemente cambiando la clase en cuestión. Por último, en el bucle de entrenamiento, querremos ser capaces de calcular la salida del modelo, gradientes y actualizar los pesos de manera automática. Vamos a ver cómo conseguirlo en las siguientes secciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBR8yjdrayqh"
      },
      "source": [
        "## MLP y Capas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTuIANJDayqi"
      },
      "source": [
        "Vamos a empezar definiendo nuestra clase `MLP`. Como ya hemos comentado, esta clase estará formada por una lista de capas y simplemente se encargará de calcular la salida del modelo aplicando cada capa de manera secuencial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:12.785890Z",
          "start_time": "2020-08-10T13:13:12.775889Z"
        },
        "id": "0Scv0kpEayqi"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers):\n",
        "        # el MLP es una lista de capas\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculamos la salida del modelo aplicando\n",
        "        # cada capa de manera secuencial\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEcgs-dsayqj"
      },
      "source": [
        "Ahora definimos las diferentes capas que necesitamos. En primer lugar tendremos una clase base que contendrá los elementos y funcionalidad común de cada capa. Esta clase contendrá una lista con los parámetros entrenables de la capa y sus gradientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.130767Z",
          "start_time": "2020-08-10T13:13:13.123088Z"
        },
        "id": "YDbcEtGLayqj"
      },
      "outputs": [],
      "source": [
        "class Layer():\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "        self.grads = []\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # por defecto, devolver los inputs\n",
        "        # cada capa hará algo diferente aquí\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad):\n",
        "        # cada capa, calculará sus gradientes\n",
        "        # y los devolverá para las capas siguientes\n",
        "        return grad\n",
        "\n",
        "    def update(self, params):\n",
        "        # si hay parámetros, los actualizaremos\n",
        "        # con lo que nos de el optimizer\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjm2tn8ayqk"
      },
      "source": [
        "Ahora podemos definir las diferentes capas que utilizaremos. Hasta ahora sólo hemos visto la capa lineal y diferentes funciones de activación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.413081Z",
          "start_time": "2020-08-10T13:13:13.396082Z"
        },
        "id": "IXnQ29XJayqk"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        # pesos de la capa\n",
        "        self.w = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(d_in+d_out)),\n",
        "                                  size=(d_in, d_out))\n",
        "        self.b = np.zeros(d_out)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        self.params = [self.w, self.b]\n",
        "        # salida del preceptrón\n",
        "        return np.dot(x, self.w) + self.b    \n",
        "    \n",
        "    def backward(self, grad_output):\n",
        "        # gradientes para la capa siguiente (BACKPROP)\n",
        "        grad = np.dot(grad_output, self.w.T)\n",
        "        self.grad_w = np.dot(self.x.T, grad_output)\n",
        "        # gradientes para actualizar pesos\n",
        "        self.grad_b = grad_output.mean(axis=0)*self.x.shape[0]\n",
        "        self.grads = [self.grad_w, self.grad_b]\n",
        "        return grad\n",
        "\n",
        "    def update(self, params):\n",
        "        self.w, self.b = params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.553974Z",
          "start_time": "2020-08-10T13:13:13.534217Z"
        },
        "id": "lyaOmDbuayqk"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad = self.x > 0\n",
        "        return grad_output*grad\n",
        "    \n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)\n",
        "\n",
        "class Sigmoid(Layer):    \n",
        "    def __call__(self, x):\n",
        "        self.x = x\n",
        "        return sigmoid(x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad = sigmoid(self.x)*(1 - sigmoid(self.x))\n",
        "        return grad_output*grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XcsoBiVayql"
      },
      "source": [
        "## Optimizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sceLphVVayql"
      },
      "source": [
        "De momento solo conocemos un algoritmo de optimización, el `descenso por gradiente`. En este algoritmo, iteraremos por todas las capas del `MLP` actualizando los parámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:13.980603Z",
          "start_time": "2020-08-10T13:13:13.975081Z"
        },
        "id": "hDTShxeHayql"
      },
      "outputs": [],
      "source": [
        "class SGD():\n",
        "    def __init__(self, net, lr):\n",
        "        self.net = net\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self):\n",
        "        for layer in self.net.layers:\n",
        "            layer.update([\n",
        "                params - self.lr*grads\n",
        "                for params, grads in zip(layer.params, layer.grads)\n",
        "            ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk5vzPrhayqm"
      },
      "source": [
        "Puedes probar algoritmos de optimización diferentes simplemente creando una nueva clase con una regla distinta para la actualización de los pesos. En futuros posts exploraremos optimizadores alternativos. Esta clase sólo se encarga de aplicar los gradientes en una manera determinada para actualizar los pesos, el cálculo de los gradientes depende de cada capa en concreto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQIO1AxAayqm"
      },
      "source": [
        "## Funciones de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JW_5gI3ayqm"
      },
      "source": [
        "Del mismo modo que hemos hecho para las capas, vamos a implementar una clase base para calcular funciones de pérdida con la funcionalidad común y después tendremos clases particulares para las diferentes funciones de pérdida que conocemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:14.563035Z",
          "start_time": "2020-08-10T13:13:14.545038Z"
        },
        "id": "-XxsDkavayqm"
      },
      "outputs": [],
      "source": [
        "class Loss():\n",
        "    def __init__(self, net):\n",
        "        self.net = net\n",
        "\n",
        "    def backward(self):\n",
        "        # derivada de la loss function con respecto \n",
        "        # a la salida del MLP\n",
        "        grad = self.grad_loss()\n",
        "        # BACKPROPAGATION\n",
        "        for layer in reversed(self.net.layers):\n",
        "            grad = layer.backward(grad)\n",
        "            \n",
        "class MSE(Loss):\n",
        "    def __call__(self, output, target):\n",
        "        self.output, self.target = output, target.reshape(output.shape)\n",
        "        loss = np.mean((self.output - self.target)**2)\n",
        "        return loss.mean()\n",
        "\n",
        "    def grad_loss(self):\n",
        "        return self.output -  self.target\n",
        "    \n",
        "class BCE(Loss):\n",
        "    def __call__(self, output, target):\n",
        "        self.output, self.target = output, target.reshape(output.shape)\n",
        "        loss = - np.mean(self.target*np.log(self.output) - (1 - self.target)*np.log(1 - self.output))\n",
        "        return loss.mean()\n",
        "\n",
        "    def grad_loss(self):\n",
        "        return self.output -  self.target\n",
        "            \n",
        "class CrossEntropy(Loss):\n",
        "    def __call__(self, output, target):\n",
        "        self.output, self.target = output, target\n",
        "        logits = output[np.arange(len(output)), target]\n",
        "        loss = - logits + np.log(np.sum(np.exp(output), axis=-1))\n",
        "        loss = loss.mean()\n",
        "        return loss\n",
        "\n",
        "    def grad_loss(self):\n",
        "        answers = np.zeros_like(self.output)\n",
        "        answers[np.arange(len(self.output)), self.target] = 1\n",
        "        return (- answers + softmax(self.output)) / self.output.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBp4uUQ4ayqn"
      },
      "source": [
        "La clase que utilicemos como función de pérdida tendrá la responsabilidad de llevar a cabo el algoritmo de `backpropagation`, propagando los gradientes desde la última capa hasta la primera para que, más tarde, el optimizador pueda actualizar los pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTaZfDYayqn"
      },
      "source": [
        "## Probando la implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T12:42:20.221071Z",
          "start_time": "2020-08-10T12:42:20.202060Z"
        },
        "id": "Uf535aq2ayqn"
      },
      "source": [
        "En este punto tenemos todas las clases que necesitamos para resolver nuestros casos de interés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aYUCL5yayqn"
      },
      "source": [
        "### Regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgFnozPMayqn"
      },
      "source": [
        "Empezamos con el caso de la regresión (en este caso directamente no lineal)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizado para manejos de directorios y rutas\n",
        "import os\n",
        "\n",
        "# Computacion vectorial y cientifica para python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Librerias para graficación (trazado de gráficos)\n",
        "from matplotlib import pyplot\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D\n",
        "import matplotlib.pyplot as plt\n",
        "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "JYrTaL89bndf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt('Dataset_Preg_2.csv', delimiter=';')\n",
        "X, y = data[:,: 8], data[:,8]\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "aLPbv_uabnGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c9f85c-6b54-43ef-d606-27e82cb2057e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(762, 8) (762,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std\n",
        "print(X_norm)\n",
        "#normalizando Y\n",
        "y_mean1, y_std1 = y.mean(axis=0), y.std(axis=0)\n",
        "y_norm = (y - y_mean1) / y_std1\n",
        "#print(\"datos de Y \")\n",
        "#print(y_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpyan6MqfV0A",
        "outputId": "b14b98f5-2fd4-4515-812e-7a0da2e66712"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.44654982 -0.19285322  1.18609689 ... -0.66913263  0.25190225\n",
            "  -0.84401395]\n",
            " [-1.35413944 -1.7701787  -0.80813099 ...  2.15568213 12.49518862\n",
            "   1.18481454]\n",
            " [-1.35413944 -1.7701787  -0.80813099 ...  2.15568213 12.49518862\n",
            "   1.18481454]\n",
            " ...\n",
            " [ 1.78781333  0.59580951 -0.4455441  ...  0.74327475 -0.01804313\n",
            "   1.18481454]\n",
            " [ 1.78781333  0.59580951 -0.4455441  ...  0.74327475 -0.01804313\n",
            "   1.18481454]\n",
            " [ 2.15745484  0.07003436 -0.45460878 ...  0.74327475  1.67077774\n",
            "   1.18481454]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPwNfihGayqo"
      },
      "source": [
        "Para ello usamos una función de activación de tipo `MSE` con una función de activación lineal en la última capa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvsbkFBoayqp"
      },
      "source": [
        "¿Y si queremos utilizar un `MLP` con más capas y más neuronas? Simplemente las añadimos a la lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:23.222303Z",
          "start_time": "2020-08-10T13:13:23.080353Z"
        },
        "id": "WyLdylzJayqp",
        "outputId": "255bac17-035d-41bb-c5ed-1aa81b5a11f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000/50000, Loss: 0.3238\n",
            "Epoch 2000/50000, Loss: 0.1897\n",
            "Epoch 3000/50000, Loss: 0.1342\n",
            "Epoch 4000/50000, Loss: 0.1046\n",
            "Epoch 5000/50000, Loss: 0.0867\n",
            "Epoch 6000/50000, Loss: 0.0743\n",
            "Epoch 7000/50000, Loss: 0.0656\n",
            "Epoch 8000/50000, Loss: 0.0590\n",
            "Epoch 9000/50000, Loss: 0.0538\n",
            "Epoch 10000/50000, Loss: 0.0496\n",
            "Epoch 11000/50000, Loss: 0.0463\n",
            "Epoch 12000/50000, Loss: 0.0435\n",
            "Epoch 13000/50000, Loss: 0.0412\n",
            "Epoch 14000/50000, Loss: 0.0391\n",
            "Epoch 15000/50000, Loss: 0.0374\n",
            "Epoch 16000/50000, Loss: 0.0358\n",
            "Epoch 17000/50000, Loss: 0.0344\n",
            "Epoch 18000/50000, Loss: 0.0331\n",
            "Epoch 19000/50000, Loss: 0.0320\n",
            "Epoch 20000/50000, Loss: 0.0310\n",
            "Epoch 21000/50000, Loss: 0.0301\n",
            "Epoch 22000/50000, Loss: 0.0293\n",
            "Epoch 23000/50000, Loss: 0.0285\n",
            "Epoch 24000/50000, Loss: 0.0278\n",
            "Epoch 25000/50000, Loss: 0.0272\n",
            "Epoch 26000/50000, Loss: 0.0266\n",
            "Epoch 27000/50000, Loss: 0.0260\n",
            "Epoch 28000/50000, Loss: 0.0255\n",
            "Epoch 29000/50000, Loss: 0.0250\n",
            "Epoch 30000/50000, Loss: 0.0245\n",
            "Epoch 31000/50000, Loss: 0.0241\n",
            "Epoch 32000/50000, Loss: 0.0237\n",
            "Epoch 33000/50000, Loss: 0.0233\n",
            "Epoch 34000/50000, Loss: 0.0230\n",
            "Epoch 35000/50000, Loss: 0.0227\n",
            "Epoch 36000/50000, Loss: 0.0224\n",
            "Epoch 37000/50000, Loss: 0.0221\n",
            "Epoch 38000/50000, Loss: 0.0218\n",
            "Epoch 39000/50000, Loss: 0.0215\n",
            "Epoch 40000/50000, Loss: 0.0213\n",
            "Epoch 41000/50000, Loss: 0.0210\n",
            "Epoch 42000/50000, Loss: 0.0208\n",
            "Epoch 43000/50000, Loss: 0.0206\n",
            "Epoch 44000/50000, Loss: 0.0204\n",
            "Epoch 45000/50000, Loss: 0.0202\n",
            "Epoch 46000/50000, Loss: 0.0200\n",
            "Epoch 47000/50000, Loss: 0.0198\n",
            "Epoch 48000/50000, Loss: 0.0196\n",
            "Epoch 49000/50000, Loss: 0.0194\n",
            "Epoch 50000/50000, Loss: 0.0193\n"
          ]
        }
      ],
      "source": [
        "D_in, H, D_out = 8, 20, 1\n",
        "# añadimos más capas\n",
        "mlp = MLP([\n",
        "    Linear(D_in, H),\n",
        "    ReLU(),\n",
        "    Linear(H, H),\n",
        "    ReLU(),\n",
        "    Linear(H, H),\n",
        "    ReLU(),\n",
        "    Linear(H, H),\n",
        "    ReLU(),\n",
        "    Linear(H, H),\n",
        "    ReLU(),\n",
        "    Linear(H, D_out)\n",
        "])\n",
        "\n",
        "optimizer = SGD(mlp, lr=0.001)\n",
        "loss = MSE(mlp)\n",
        "\n",
        "epochs = 50000\n",
        "batch_size = 100\n",
        "\n",
        "batches = len(X) // batch_size\n",
        "log_each = 1000\n",
        "l = []\n",
        "for e in range(1,epochs+1):\n",
        "    _l = []\n",
        "    for b in range(batches):\n",
        "        x = X_norm[b*batch_size:(b+1)*batch_size]\n",
        "        y = y_norm[b*batch_size:(b+1)*batch_size] \n",
        "        y_pred = mlp(x)    \n",
        "        _l.append(loss(y_pred, y))\n",
        "        loss.backward()    \n",
        "        optimizer.update()\n",
        "    l.append(np.mean(_l))\n",
        "    if not e % log_each:\n",
        "        print(f'Epoch {e}/{epochs}, Loss: {np.mean(l):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new1 = [34,2013,2800,4,2,3,88000,1]\n",
        "x_new2 = [36,2015,4000,4,2,3,130000,3]\n",
        "x_new3 = [19,2017,2300,5,2,2,48000,1]\n",
        "x_new = [2,2018,1800,5,1,3,60000,1]\n",
        "\n",
        "#x_new1 = (x_new1 - X_mean) / X_std\n",
        "y_pred1 = mlp(x_new1)\n",
        "print(y_pred1 )\n",
        "#x_new1 = (x_new1 - X_mean) / X_std \n",
        "y_pred2 = mlp(x_new2)\n",
        "print(y_pred2 )\n",
        "#x_new3 = (x_new3 - X_mean) / X_std \n",
        "y_pred3 = mlp(x_new3)\n",
        "print(y_pred3 )\n",
        "#x_new = (x_new - X_mean) / X_std\n",
        "y_pred = mlp(x_new)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_JxdjcBhkkH",
        "outputId": "049518ad-8949-4b58-e90b-ec03716cb0e0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-14106.81179314]\n",
            "[-20877.686367]\n",
            "[-7943.703436]\n",
            "[-9553.78704039]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rAftJBOhljqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r6xCx_BTljb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_new1 = [2,2013,4000,4,3,3,136000,1]\n",
        "\n",
        "w = mlp.l[-1]\n",
        "x_new1 = (x_new1 - X_mean) / X_std\n",
        "y_pred = mlp.predict(w, x_new1)\n",
        "print(y_pred * y_mean1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "iYOpg75qhkaH",
        "outputId": "bf91c128-17f4-475b-9bc9-0f58c10c983e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-1d872edef1be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_new1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m136000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_new1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_new1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'l'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yicCudGghkLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-10T13:13:24.848984Z",
          "start_time": "2020-08-10T13:13:24.740752Z"
        },
        "id": "JNru1OCsayqp",
        "outputId": "79898a38-f090-41b5-e49a-b1d555fa6dc5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x_new = np.linspace(-3, 3, 100)\n",
        "x_new = x_new.reshape(len(x_new),1)\n",
        "y_pred = mlp(x_new)\n",
        "    \n",
        "plt.plot(X, Y, \"b.\")\n",
        "plt.plot(x_new, y_pred, \"-k\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LdC8303ayqq"
      },
      "source": [
        "Nuestro framework es capaz de llevar a cabo la tarea de regresión, y si ahora queremos llevar a cabo tareas de clasificación es tan sencillo como cambiar la función de pérdida y de activación en la clase `MLP`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsQ1C0iWayqv"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snTjYKfWayqv"
      },
      "source": [
        "En este post hemos visto como podemos implementar nuestro propio framework de perceptrón multicapa. Las ideas expuestas son muy similares a las utilizadas internamente por frameworks de `redes neuronales` como `Pytorch` o `Tensorflow`, lo cual nos permitirá conocer en detalle como funcionan estos frameworks a la hora de trabajar con arquitecturas más complicadas. Nuestro framework es flexible, ya que con la misma interfaz es capaz de llevar a cabo las diferentes tareas y de manera muy sencilla podemos probar diferentes arquitecturas de `MLP` con más o menos capas, diferentes funciones de pérdida y de activación, etc. En este [post](https://sensioai.com/blog/017_clasificacion_multiclase) utilizamos el dataset MNIST para clasificación de imágenes, ¿te ves capaz de utilizar nuestro framework para esta tarea?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "MLP_Red_Neu_Ejecio_2_final.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}